# baseline_ollama.py
import time
from openai import OpenAI # ‡πÉ‡∏ä‡πâ library ‡∏Ç‡∏≠‡∏á OpenAI ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ö Ollama

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Ollama ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á
try:
    client = OpenAI(
        base_url='http://localhost:11434/v1',
        api_key='ollama', # ‡πÉ‡∏™‡πà 'ollama'
    )
    client.models.list()
    print("‚úÖ Baseline (Ollama) ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
except Exception as e:
    print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama (http://localhost:11434)")
    print("üëâ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏£‡∏±‡∏ô 'ollama serve' ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° Ollama ‡πÅ‡∏•‡πâ‡∏ß")
    client = None

# ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ (‡∏ï‡πâ‡∏≠‡∏á 'ollama pull codellama:7b-instruct' ‡∏Å‡πà‡∏≠‡∏ô)
# ‡∏´‡∏£‡∏∑‡∏≠ 'ollama pull llama3:8b'
MODEL_NAME = "codellama:7b-instruct"

def generate_code_ollama(problem_prompt: str) -> dict:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Ollama (CodeLlama) ‡πÅ‡∏ö‡∏ö One-shot
    """
    if not client:
        return {
            "code": "ERROR: Ollama client not initialized",
            "latency_sec": 0,
            "tokens_used": 0
        }

    system_prompt = "You are an expert Python programmer. Respond ONLY with the raw Python code (no markdown, no explanations)."
    
    start_time = time.time()
    
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Problem:\n{problem_prompt}"}
            ],
            temperature=0.1,
        )
        
        generated_code = response.choices[0].message.content
        tokens_used = response.usage.total_tokens
        
    except Exception as e:
        print(f"!! Error (Ollama): {e}")
        generated_code = f"ERROR: {e}"
        tokens_used = 0
        
    end_time = time.time()
    latency_sec = end_time - start_time
    
    # ‡πÅ‡∏Å‡∏∞‡πÇ‡∏Ñ‡πâ‡∏î
    if "```python" in generated_code:
        generated_code = generated_code.split("```python")[1].split("```")[0]
    elif "```" in generated_code:
        generated_code = generated_code.split("```")[1].split("```")[0]

    return {
        "code": generated_code.strip(),
        "latency_sec": latency_sec,
        "tokens_used": tokens_used
    }

if __name__ == "__main__":
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    test_problem = "def add(a, b):\n    \"\"\"Return the sum of two numbers.\"\"\""
    result = generate_code_ollama(test_problem)
    print("--- Baseline (Ollama) Test ---")
    print(f"Time taken: {result['latency_sec']:.2f}s")
    print(f"Tokens used: {result['tokens_used']}")
    print("--- Code ---")
    print(result['code'])