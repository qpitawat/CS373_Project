# AI Code Generation Evaluation Framework

à¹€à¸Ÿà¸£à¸¡à¹€à¸§à¸´à¸£à¹Œà¸à¸ªà¸³à¸«à¸£à¸±à¸šà¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¹à¸¥à¸°à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸„à¸§à¸²à¸¡à¸ªà¸²à¸¡à¸²à¸£à¸–à¹ƒà¸™à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸„à¹‰à¸”à¸‚à¸­à¸‡ AI models à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ HumanEval benchmark (164 problems)

---

## ğŸ“‹ à¸ à¸²à¸à¸£à¸§à¸¡

à¹‚à¸›à¸£à¹€à¸ˆà¸„à¸™à¸µà¹‰à¸—à¸”à¸ªà¸­à¸š 3 à¹à¸™à¸§à¸—à¸²à¸‡à¹ƒà¸™à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸„à¹‰à¸”:

1. **Agentic AI System** - Multi-agent collaborative system (3 agents à¸—à¸³à¸‡à¸²à¸™à¸£à¹ˆà¸§à¸¡à¸à¸±à¸™)
2. **Optimized Prompt** - Single model + prompt engineering
3. **Poor Prompt** - Single model + simple prompt
4. **Cloud APIs** - Gemini, ChatGPT, Claude

---

## ğŸ¤– Agentic AI System Architecture

### Agent Roles

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENTIC AI WORKFLOW                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[Problem] â†’ [Generator] â†’ [Draft Code v1]
                              â†“
                         [Manager Review]
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                   â”‚
              [PERFECT?]            [Issues Found]
                    â”‚                   â”‚
                    â†“                   â†“
              [APPROVED]           [Refiner Fix]
                                        â†“
                                  [Draft Code v2]
                                        â†“
                                  [Manager Review]
                                        â†“
                                  (à¸§à¸™à¸‹à¹‰à¸³ max 3 à¸£à¸­à¸š)
```

### 3 Agents à¹à¸¥à¸°à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆ

**1. Generator Agent (Qwen3 8B)**
- **à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆ:** à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸„à¹‰à¸”à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸ˆà¸²à¸ problem statement
- **Prompt:** Optimized prompt à¸à¸£à¹‰à¸­à¸¡ edge case handling
- **Output:** Draft code version 1

**2. Manager Agent (DeepSeek Coder 6.7B)**
- **à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆ:** à¸£à¸µà¸§à¸´à¸§à¹‚à¸„à¹‰à¸”à¹à¸¥à¸°à¸«à¸² logic errors
- **Focus:** Off-by-one errors, missing edge cases, wrong algorithms
- **Output:** 
  - `"PERFECT"` â†’ à¸­à¸™à¸¸à¸¡à¸±à¸•à¸´à¹‚à¸„à¹‰à¸”
  - `"Fix: [issue]"` â†’ à¸ªà¹ˆà¸‡à¸•à¹ˆà¸­à¹ƒà¸«à¹‰ Refiner à¹à¸à¹‰à¹„à¸‚

**3. Refiner Agent (Llama3.1 8B)**
- **à¸«à¸™à¹‰à¸²à¸—à¸µà¹ˆ:** à¹à¸à¹‰à¹„à¸‚à¹‚à¸„à¹‰à¸”à¸•à¸²à¸¡ feedback à¸ˆà¸²à¸ Manager
- **Focus:** Fix bugs, add edge cases, maintain function signature
- **Output:** Improved code version

### Workflow Steps

```python
# Iteration 0: Initial Generation
Generator â†’ Draft Code v1

# Iteration 1-3: Refinement Loop
for i in range(max_iterations):
    Manager reviews Draft Code
    
    if "PERFECT" in feedback:
        APPROVED â†’ Return final code
        break
    
    Refiner fixes issues â†’ Draft Code v(i+1)
    Manager reviews again
```

### à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡ Output

```
AGENTIC TEAM: Starting (Max 3 iterations)
   [Generator working...]
   [Iteration 0] Draft complete (Time: 8.54s, Tokens: 512)

[Iteration 1]
   [Manager reviewing...]
   [Manager] Fix: Missing edge case for empty list input
   [Refiner working...]
   [Refiner] Code updated (Time: 6.23s, Tokens: 384)

[Iteration 2]
   [Manager reviewing...]
   [Manager] APPROVED
   
AGENTIC TEAM: Complete (Total: 18.45s, 1248 tokens)
```

---

## ğŸ”„ Flow à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š

### 1. Evaluation Flow (eval_humanevalOpt.py)

```
START
  â†“
Load HumanEval Dataset (164 problems)
  â†“
For each problem:
  â”œâ”€â†’ [1] Call Agentic AI System
  â”‚     â”œâ”€ Generator creates draft
  â”‚     â”œâ”€ Manager reviews (loop)
  â”‚     â””â”€ Refiner fixes issues
  â”‚
  â”œâ”€â†’ [2] Extract generated code
  â”‚
  â”œâ”€â†’ [3] Execute code with test cases
  â”‚     â”œâ”€ Create temp file
  â”‚     â”œâ”€ Run subprocess (5s timeout)
  â”‚     â””â”€ Check pass/fail
  â”‚
  â”œâ”€â†’ [4] Measure 9 metrics
  â”‚     â”œâ”€ Correctness (pass/fail)
  â”‚     â”œâ”€ Latency (seconds)
  â”‚     â”œâ”€ Tokens used
  â”‚     â”œâ”€ Lines of Code (LOC)
  â”‚     â”œâ”€ Cyclomatic Complexity
  â”‚     â”œâ”€ Has Imports (boolean)
  â”‚     â”œâ”€ Has Docstring (boolean)
  â”‚     â”œâ”€ Syntax Valid (boolean)
  â”‚     â””â”€ Execution Speed (ns)
  â”‚
  â””â”€â†’ [5] Write results to CSV
  
END â†’ Save results/humanevalOpt_[timestamp].csv
```

### 2. Test Execution Flow

```python
# à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸•à¹ˆà¸¥à¸° problem:

# Step 1: Generate Code
result = run_agentic_team(problem_prompt)
generated_code = result["code"]
latency = result["latency_sec"]
tokens = result["tokens_used"]

# Step 2: Test Correctness
with tempfile.NamedTemporaryFile(mode='w', suffix='.py') as f:
    f.write(generated_code)
    f.write("\n\n")
    f.write(problem['test'])  # Test cases from HumanEval
    f.write(f"\ncheck({problem['entry_point']})")
    
    # Run with 5-second timeout
    result = subprocess.run(
        ['python', temp_file],
        capture_output=True,
        timeout=5
    )
    
    passed = (result.returncode == 0)

# Step 3: Measure Code Quality
loc = count_lines_of_code(generated_code)
complexity = calculate_cyclomatic_complexity(generated_code)
has_imports = check_imports(generated_code)
syntax_valid = check_syntax(generated_code)
exec_time = measure_execution_speed(generated_code)

# Step 4: Save to CSV
csv_writer.writerow({
    'problem_id': problem['task_id'],
    'system_name': 'Agentic_AI',
    'passed_test': passed,
    'generation_latency_sec': latency,
    'total_tokens_used': tokens,
    'loc': loc,
    'cyclomatic_complexity': complexity,
    ...
})
```

### 3. Metrics Measurement Flow

```
Generated Code
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  METRIC MEASUREMENT PIPELINE        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚
â”‚  [1] Correctness                    â”‚
â”‚      â””â”€ subprocess.run() â†’ pass/failâ”‚
â”‚                                     â”‚
â”‚  [2] Latency                        â”‚
â”‚      â””â”€ time.time() â†’ seconds       â”‚
â”‚                                     â”‚
â”‚  [3] Tokens                         â”‚
â”‚      â””â”€ Ollama API â†’ count          â”‚
â”‚                                     â”‚
â”‚  [4] LOC                            â”‚
â”‚      â””â”€ Line counting â†’ lines       â”‚
â”‚                                     â”‚
â”‚  [5] Complexity                     â”‚
â”‚      â””â”€ AST analysis â†’ score        â”‚
â”‚                                     â”‚
â”‚  [6] Has Imports                    â”‚
â”‚      â””â”€ AST parsing â†’ boolean       â”‚
â”‚                                     â”‚
â”‚  [7] Has Docstring                  â”‚
â”‚      â””â”€ AST parsing â†’ boolean       â”‚
â”‚                                     â”‚
â”‚  [8] Syntax Valid                   â”‚
â”‚      â””â”€ ast.parse() â†’ boolean       â”‚
â”‚                                     â”‚
â”‚  [9] Execution Speed                â”‚
â”‚      â””â”€ timeit â†’ nanoseconds        â”‚
â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
Write to CSV
```

---

## ğŸ“Š Evaluation Metrics

### Primary Metrics

| Metric | à¸§à¸´à¸˜à¸µà¸§à¸±à¸” | à¸«à¸™à¹ˆà¸§à¸¢ | à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¸”à¸µ |
|--------|---------|-------|----------|
| **Correctness** | subprocess.run() | Boolean | True |
| **Latency** | time.time() | Seconds | à¸•à¹ˆà¸³ |
| **Tokens** | Ollama API | Count | à¸•à¹ˆà¸³ |

### Code Quality Metrics

| Metric | à¸§à¸´à¸˜à¸µà¸§à¸±à¸” | à¸«à¸™à¹ˆà¸§à¸¢ | à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¸”à¸µ |
|--------|---------|-------|----------|
| **LOC** | Line counting | Lines | à¸•à¹ˆà¸³ (à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¹€à¸à¸´à¸™à¹„à¸›) |
| **Complexity** | AST analysis | Score | à¸•à¹ˆà¸³ |
| **Has Imports** | AST parsing | Boolean | True |
| **Has Docstring** | AST parsing | Boolean | True |
| **Syntax Valid** | ast.parse() | Boolean | True |
| **Exec Speed** | timeit | Nanoseconds | à¸•à¹ˆà¸³ |

---

## ğŸš€ à¸à¸²à¸£à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡

### 1. à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ Dependencies

```bash
pip install -r requirements.txt
```

**Dependencies:**
- `openai` - à¸ªà¸³à¸«à¸£à¸±à¸š Ollama API à¹à¸¥à¸° ChatGPT
- `google-generativeai` - à¸ªà¸³à¸«à¸£à¸±à¸š Gemini
- `anthropic` - à¸ªà¸³à¸«à¸£à¸±à¸š Claude
- `python-dotenv` - à¸ªà¸³à¸«à¸£à¸±à¸š .env

### 2. à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ Ollama à¹à¸¥à¸° Models

```bash
# à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ Ollama
# https://ollama.ai

# à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸” models à¸ªà¸³à¸«à¸£à¸±à¸š Agentic AI
ollama pull qwen3:8b                       # Generator (5.2 GB)
ollama pull llama3.1:8b                    # Refiner (4.9 GB)
ollama pull deepseek-coder:6.7b-instruct   # Manager (3.8 GB)

# (Optional) Models à¸ªà¸³à¸«à¸£à¸±à¸š Poor Prompt testing
ollama pull phi3:3.8b                      # 2.2 GB
ollama pull gemma:7b-instruct              # 5.0 GB
ollama pull mistral:7b-instruct            # 4.4 GB

# à¹€à¸£à¸´à¹ˆà¸¡ Ollama service
ollama serve
```

### 3. à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² API Keys (Optional - à¸ªà¸³à¸«à¸£à¸±à¸š Cloud APIs)

```bash
# Copy .env.example
cp .env.example .env

# à¹à¸à¹‰à¹„à¸‚ .env
GOOGLE_API_KEY=your-gemini-key-here
OPENAI_API_KEY=your-chatgpt-key-here
ANTHROPIC_API_KEY=your-claude-key-here
```

**à¸‚à¸­ API Keys:**
- Gemini: https://aistudio.google.com/app/apikey
- ChatGPT: https://platform.openai.com/api-keys
- Claude: https://console.anthropic.com/settings/keys

---

## ğŸ’» à¸§à¸´à¸˜à¸µà¹ƒà¸Šà¹‰à¸‡à¸²à¸™

### 1. à¸—à¸”à¸ªà¸­à¸š Agentic AI System

```bash
python eval_humanevalOpt.py
```

**Output:**
```
Ollama ready
Loading HumanEval...
HumanEval Evaluation
Problems: 164 | Systems: 1
Results: results/humanevalOpt_20251116_180610.csv

[1/164] HumanEval/0
AGENTIC TEAM: Starting (Max 3 iterations)
   [Generator working...]
   [Iteration 0] Draft complete (Time: 8.54s, Tokens: 512)
[Iteration 1]
   [Manager reviewing...]
   [Manager] APPROVED
AGENTIC TEAM: Complete (Total: 12.34s, 768 tokens)
Agentic_AI... Passed: True | Latency: 12.34s | Tokens: 768 | LOC: 15 | Complexity: 3

[2/164] HumanEval/1
...
```

### 2. à¸—à¸”à¸ªà¸­à¸š Poor Prompt (6 Local Models)

```bash
python eval_humanevalPoor.py
```

**Models à¸—à¸µà¹ˆà¸—à¸”à¸ªà¸­à¸š:**
- DeepSeek Coder 6.7B
- Llama 3.1 8B
- Phi3 3.8B
- Qwen3 8B
- Gemma 7B
- Mistral 7B

### 3. à¸—à¸”à¸ªà¸­à¸š Cloud APIs

```bash
python eval_humanevalAi.py
```

**APIs à¸—à¸µà¹ˆà¸—à¸”à¸ªà¸­à¸š:**
- Google Gemini 2.5 Flash
- OpenAI GPT-4o
- Anthropic Claude 3.5 Sonnet

**à¸«à¸¡à¸²à¸¢à¹€à¸«à¸•à¸¸:** à¸¡à¸µ rate limiting 5 à¸§à¸´à¸™à¸²à¸—à¸µà¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ requests

### 4. à¸—à¸”à¸ªà¸­à¸š Agentic AI à¹à¸šà¸šà¹€à¸”à¸µà¹ˆà¸¢à¸§

```bash
python agentic_ai.py
```

---

## ğŸ“ à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸›à¸£à¹€à¸ˆà¸„

```
project/
â”‚
â”œâ”€â”€ eval_humanevalOpt.py          # à¸—à¸”à¸ªà¸­à¸š Agentic AI
â”œâ”€â”€ eval_humanevalPoor.py         # à¸—à¸”à¸ªà¸­à¸š Poor Prompt (6 models)
â”œâ”€â”€ eval_humanevalAi.py           # à¸—à¸”à¸ªà¸­à¸š Cloud APIs
â”œâ”€â”€ run_evaluation.py             # Legacy script
â”‚
â”œâ”€â”€ agentic_ai.py                 # â­ Multi-agent system
â”‚   â”œâ”€â”€ code_generator_agent()   # Generator (Qwen3 8B)
â”‚   â”œâ”€â”€ manager_reviewer_agent() # Manager (DeepSeek 6.7B)
â”‚   â”œâ”€â”€ code_refiner_agent()     # Refiner (Llama3.1 8B)
â”‚   â””â”€â”€ run_agentic_team()       # Main orchestrator
â”‚
â”œâ”€â”€ baseline_gemini.py            # Gemini API wrapper
â”œâ”€â”€ baseline_chatgpt.py           # ChatGPT API wrapper
â”œâ”€â”€ baseline_claude.py            # Claude API wrapper
â”‚
â”œâ”€â”€ optPrompt/                    # Optimized prompt versions
â”‚   â”œâ”€â”€ baseline_deepseek.py
â”‚   â”œâ”€â”€ baseline_llama31.py
â”‚   â”œâ”€â”€ baseline_phi3.py
â”‚   â”œâ”€â”€ baseline_qwen.py
â”‚   â”œâ”€â”€ baseline_gemma.py
â”‚   â””â”€â”€ baseline_mistral.py
â”‚
â”œâ”€â”€ poorPrompt/                   # Poor prompt versions
â”‚   â”œâ”€â”€ baseline_deepseek.py
â”‚   â”œâ”€â”€ baseline_llama31.py
â”‚   â”œâ”€â”€ baseline_phi3.py
â”‚   â”œâ”€â”€ baseline_qwen.py
â”‚   â”œâ”€â”€ baseline_gemma.py
â”‚   â””â”€â”€ baseline_mistral.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ human-eval-v2-20210705.jsonl  # 164 problems
â”‚
â”œâ”€â”€ results/                      # CSV outputs
â”‚   â”œâ”€â”€ humanevalOpt_*.csv       # Agentic AI results
â”‚   â”œâ”€â”€ humanevalPoor_*.csv      # Poor prompt results
â”‚   â””â”€â”€ humanevalAi_*.csv        # Cloud API results
â”‚
â”œâ”€â”€ .env.example                  # API keys template
â”œâ”€â”€ .env                          # Your API keys (gitignored)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“ˆ à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ (CSV Format)

### Columns

```csv
problem_id,system_name,generated_code,passed_test,generation_latency_sec,total_tokens_used,loc,cyclomatic_complexity,has_imports,has_docstring,syntax_valid,num_functions,avg_exec_time_ns
HumanEval/0,Agentic_AI,"def has_close_elements...",True,12.34,768,15,3,True,True,True,1,1250000
HumanEval/1,Agentic_AI,"def separate_paren...",True,15.67,892,22,5,True,True,True,1,980000
```

### à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ

| Problem | System | Passed | Latency | Tokens | LOC | Complexity |
|---------|--------|--------|---------|--------|-----|------------|
| HumanEval/0 | Agentic_AI | âœ… True | 12.34s | 768 | 15 | 3 |
| HumanEval/1 | Agentic_AI | âœ… True | 15.67s | 892 | 22 | 5 |
| HumanEval/2 | Agentic_AI | âŒ False | 18.23s | 1024 | 28 | 7 |

---

## âš™ï¸ Configuration

### Model Parameters

```python
# Temperature
temperature = 0.1  # à¸„à¸§à¸²à¸¡à¸ªà¸¡à¹ˆà¸³à¹€à¸ªà¸¡à¸­à¸ªà¸¹à¸‡

# Timeout
test_timeout = 5  # à¸§à¸´à¸™à¸²à¸—à¸µ

# Max Iterations (Agentic AI)
max_iterations = 3  # à¸£à¸­à¸š

# Rate Limiting (Cloud APIs)
delay_between_requests = 5  # à¸§à¸´à¸™à¸²à¸—à¸µ
```

### Agentic AI Configuration

```python
# agentic_ai.py
MODEL_GENERATOR = "qwen3:8b"                    # Generator
MODEL_REFINER = "llama3.1:8b"                   # Refiner  
MODEL_MANAGER = "deepseek-coder:6.7b-instruct"  # Manager

# Workflow
max_iterations = 3  # à¸ˆà¸³à¸™à¸§à¸™à¸£à¸­à¸š refinement à¸ªà¸¹à¸‡à¸ªà¸¸à¸”
```

---

## ğŸ”§ à¸à¸²à¸£à¹à¸à¹‰à¸›à¸±à¸à¸«à¸²

### Ollama Connection Error

```bash
# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š Ollama service
ollama serve

# à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š models
ollama list

# à¸—à¸”à¸ªà¸­à¸š connection
curl http://localhost:11434
```

### API Key Expired

```bash
# à¸ªà¸£à¹‰à¸²à¸‡ API key à¹ƒà¸«à¸¡à¹ˆ
# Gemini: https://aistudio.google.com/app/apikey
# ChatGPT: https://platform.openai.com/api-keys
# Claude: https://console.anthropic.com/settings/keys

# à¸­à¸±à¸à¹€à¸”à¸—à¹ƒà¸™ .env
GOOGLE_API_KEY=your-new-key
```

### Model Not Found

```bash
# à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸” models à¸—à¸µà¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™
ollama pull qwen3:8b
ollama pull llama3.1:8b
ollama pull deepseek-coder:6.7b-instruct
```

### à¸—à¸”à¸ªà¸­à¸šà¹à¸šà¸šà¹€à¸£à¹‡à¸§ (Subset)

```python
# à¹à¸à¹‰à¹„à¸‚à¹ƒà¸™ eval_*.py
def load_human_eval(limit: int = None):
    # ...
    return problems[:10]  # à¸—à¸”à¸ªà¸­à¸šà¹à¸„à¹ˆ 10 problems
```

---

## ğŸ“Š à¹€à¸§à¸¥à¸²à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¹ƒà¸™à¸à¸²à¸£à¸—à¸”à¸ªà¸­à¸š

| Script | Models | Problems | à¹€à¸§à¸¥à¸²à¹‚à¸”à¸¢à¸›à¸£à¸°à¸¡à¸²à¸“ |
|--------|--------|----------|----------------|
| `eval_humanevalOpt.py` | Agentic AI (3 agents) | 164 | ~30-40 à¸™à¸²à¸—à¸µ |
| `eval_humanevalPoor.py` | 6 local models | 164 | ~60-80 à¸™à¸²à¸—à¸µ |
| `eval_humanevalAi.py` | 3 cloud APIs | 164 | ~20-30 à¸™à¸²à¸—à¸µ |

**à¸«à¸¡à¸²à¸¢à¹€à¸«à¸•à¸¸:**
- Agentic AI à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸²à¸™à¸²à¸™à¸à¸§à¹ˆà¸²à¹€à¸à¸£à¸²à¸°à¸¡à¸µà¸«à¸¥à¸²à¸¢à¸£à¸­à¸š refinement
- Cloud APIs à¸¡à¸µ rate limiting (5s delay)
- à¹€à¸§à¸¥à¸²à¸‚à¸¶à¹‰à¸™à¸­à¸¢à¸¹à¹ˆà¸à¸±à¸š hardware à¹à¸¥à¸° network

---

## ğŸ¯ à¸„à¸§à¸²à¸¡à¹à¸•à¸à¸•à¹ˆà¸²à¸‡à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ Prompts

### Optimized Prompt

```python
system_prompt = """You are a world-class Python programmer competing in a coding challenge.

YOUR MISSION: Write PERFECT, PRODUCTION-READY code that passes ALL test cases.

MANDATORY REQUIREMENTS:
1. Include ALL necessary imports (typing, re, math, heapq, collections, itertools, etc.)
2. Follow the EXACT function signature from the problem
3. Handle ALL edge cases:
   - Empty inputs ([], "", None)
   - Single element inputs
   - Negative numbers
   - Zero values
   - Large inputs
4. Write efficient O(n) or O(n log n) solutions when possible
5. Use appropriate data structures (dict, set, deque, heap)
6. NO explanations, NO test code, NO print/input statements

OUTPUT: Raw Python code ONLY. No markdown, no comments except docstring."""
```

### Poor Prompt

```python
system_prompt = "You are an expert Python programmer. Respond ONLY with the raw Python code (no markdown, no explanations)."
```

### à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸—à¸µà¹ˆà¸„à¸²à¸”à¸«à¸§à¸±à¸‡

- **Optimized Prompt:** Pass rate à¸ªà¸¹à¸‡à¸à¸§à¹ˆà¸², handle edge cases à¸”à¸µà¸à¸§à¹ˆà¸²
- **Poor Prompt:** à¸­à¸²à¸ˆà¸¡à¸µà¸›à¸±à¸à¸«à¸² missing imports, edge cases
- **Agentic AI:** Pass rate à¸ªà¸¹à¸‡à¸ªà¸¸à¸” à¹€à¸à¸£à¸²à¸°à¸¡à¸µ review à¹à¸¥à¸° refinement loop

---

## ğŸ“ License

à¹‚à¸›à¸£à¹€à¸ˆà¸„à¸™à¸µà¹‰à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸¨à¸¶à¸à¸©à¸²à¹à¸¥à¸°à¸§à¸´à¸ˆà¸±à¸¢
